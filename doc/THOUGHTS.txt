ON WHAT WOULD LATER BECOME THE 'EXTENSION' TRAIT
the tokenizer turns a buffer into a stream of grouped caracteres (or a word for
short), which have no indication of which line/column they occurred

since the lexer has no positional information of each word, it has to somehow
'know' what each caracter is in order to make a token out of it

this design inherently forces the lexer to 'know' what each word is. My attempt
to create a Classifier trait is one way of creating common grounds for doing just that. But this creates a problem
when it comes down to extending the support of the lexer for different opcodes.

You see, the code required for the lexer becomes proportional with the number
of words which have to be mapped to a token.

This creates the need for rewritting parts of the lexer whenever new extensions
of the riscv specification are added/deprecated/changed

Not only that, but later on we also have to somehow spot/tag important tokens among
those the lexer has generated. This is not that hard, since those tokens will
be enums, whose important variants can be pattern matched

At some point, these opcodes will have to somehow provide what they are actually
intended for (summing, subtracting, reading from, writing to) and how they
should be used (1 parameter, no params, two parameters, ...)

if we interpret that an extension is a vector/collection of opcodes, we could
create a trait which would force implementors to define methods to map a string
slice to an opcode. But then, all implementations of Lexers would have to
implement those interfaces/traits in order to bring bridge the lexical space of
the language to the functionality expected for each opcode

we could create something which is independent from the actual string
representing that opcode. Then the implementors job would only be to map the
string slice to the entity for that extension.



ON HOW TO TRANSLATE PSEUDO INSTRUCTIONS
The toplevel trait to be implemented by all lexers states that the return value
for the 'parse' method should be a Vec<Self::Token>.The next step after
classifying the tokens is to group them into instructions to be later executed.
This includes opcodes+arguments, pseudoinst+arguments, label-to-offset
translation, and more. The way this was thought to be done was to turn the
Vec<Self::Token> into a Vec<Vec<Self::Token>>, that is, each element is a
vector of tokens/a instruction.

The only problem arises when doing something with those instructions, more
specifically with the translation of the pseudo instruction into their real
opcodes. The reason for that is because we have to somehow have the ability of
copying multiple times the arguments of pseudo instruction . This is because
pseudo instruction might reuse more than once one of its arguments during the
decode. Well, in rust we can only copy data for types which implement Copy, but
we can't implement the Copy trait for Trait Objects (such as the 'dyn
Extension'), because those aren't compatible with the Sized Trait (which by
itself is necessary to implement Copy). Okay, that's bad. One workaround for
this is to implement the 'Clone' trait instead of the 'Copy' trait and use the
'clone()' method for copying. That's exactly what we did!

But we could've done it differently tho. You see, we don't need to copy/clone
the 'Self::Token' by itself. If we designed the translation method for
pseucodes to be something as: 'translate(&Vec<Self::Token>) -> Vec<(Box<dyn
Extension>, Vec<Arg>)', we wouldn't need to copy the tokens, but still thats
kind of problematic, because this would require turning tokens into such tuples
in at least two different places in the code.



ON HOW ASSEMBLY-TO-ELF COULD WORK
design 0
    //0. &str
    // -> 1. Vec<String>
    // -> 2. Vec<Token>
    //      -> 2.1 Vec<Vec<Token>> (+pseudos)
    //      -> 2.2 Vec<Vec<Token>> (-pseudos)->
    // -> 3. Vec<(Box<dyn Extension>, Vec<ArgValue>)>
    // -> 4. Vec<u32>

    //0 -> 1: pattern recognition (through something regex-like)
    //1 -> 2: string classification (strategy: common type map)
    //2 -> 3:
    //3 -> 4: 

design 1
    //0. &str
    // -> 1. Vec<String>
    // -> 2. Vec<Token>
    //    -> 2.1 Vec<Vec<Token>> (+pseudos)
    //    -> 2.2 Vec<(Rc<dyn Extension> | Label | Section, Vec<NUMBER i32 | REGISTER i32 | OFFSET i32 | LABEL (String) >)> (+pseudos, +symb addresses)
    //    -> 2.3 Vec<(Rc<dyn Extension> | Label | Section, Vec<NUMBER i32 | REGISTER i32 | OFFSET i32 | LABEL (String) >)> (-pseudos, +symb addresses)
    //    -> 2.4 Vec<((Rc<dyn Extension> | Label | Section, Addr), Vec<NUMBER i32 | REGISTER i32 | OFFSET i32 | LABEL (i32) >)> (-pseudos, -symb addresses)
    // -> 3. Vec<(Box<dyn Extension>, Vec<ArgValue>)>
    // -> 4. Vec<u32>

    //0 -> 1: pattern recognition (through something regex-like)
    //1 -> 2: string classification (strategy: common type map)
    //2 -> 2.1: this could be done by introducing a new token: END OF LINE
    //2.1 -> 2.2: instantiate a 'dyn Extension' inst only once and reuse it (Maybe do the same for Labels?)
    //2.2 -> 2.3: copying any arg type will be trivial (no implementation of 'Clone' for 'dyn Extension' needed)
    //2.3 -> 2.4: string-to-integer symbolic labeling resolution
    //2 -> 3:
    //3 -> 4: 

design 2
    //0. &str
    // -> 1. Vec<String>
    // -> 2. Vec<Token>
    //    -> 2.1 Vec<Vec<Token>>
    //    -> 2.2 Vec<
			Label |
			(Section, ".data" | ".text") |
			(Pseudo, Vec<ARG>) |
			(Directive, Vec<ARG + StringLiteral>) |
			(Rc<dyn Extension>, Vec<ARG>)
		 >
		 ARG = NUMBER i32 | REGISTER i32 | OFFSET i32 | LABEL (String)
    //    -> 2.3 Vec<
			Label |
			(Section, ".data" | ".text") |
			(Directive, Vec<ARG + StringLiteral>) |
			(Rc<dyn Extension>, Vec<ARG>)
		 > (pseudo-to-instruction rewrite)
    //    -> 2.4 Vec<
			Label |
			(Section, ".data" | ".text") |
			(? (Insert, Special) RawBytes) |
			(Rc<dyn Extension>, Vec<ARG>)
		 > (directives might turn into raw bytes or might become a metadata for the assembler (start address))
    //    -> 2.5 Vec<
			Label |
			(Section, ".data" | ".text") |
			(? RawBytes) |
			(Rc<dyn Extension>, Vec<ARG>)
		 > (symbolic-name-to-addr resolution)
    //    -> 2.6 Vec<
			( Label |
			  (Section, ".data" | ".text") |
			  (? RawBytes) |
			  (Rc<dyn Extension>, Vec<ARG>)
			, Addr)
		 > (Everything gains a starting address)
    // -> 3. Vec<AssemblerInstruction>
    // -> 4. Vec<u32>

    //0 -> 1: pattern recognition (through something regex-like)
    //1 -> 2: string classification (strategy: common type map)
    //2 -> 2.1: this could be done by introducing a new token: END OF LINE
    //2.1 -> 2.2: instantiate a 'dyn Extension' inst only once and reuse it (Maybe do the same for Labels?)
    //2.2 -> 2.3: copying any arg type will be trivial (no implementation of 'Clone' for 'dyn Extension' needed)
    //2.3 -> 2.4: string-to-integer symbolic labeling resolution
    //2 -> 3:
    //3 -> 4: 



ON WHAT WOULD LATER BECOME THE 'Pseudo' and 'Directive' TRAITS
Criar trait object para pseudo e para directive (que sera uma directiva
especifica de inserção de bytes raw)

O trait do pseudo vai ser definido para um trait type Token com no minimo o
seguinte metodo:
	fn translate(&self, Keyword, Args) -> Vec<(Keyword, Args)>

Caso o tipo implementado para o pseudo seja um enum, entao todas variantes
terao que funcionar para o tal metodo

Alem disso, sera necessário uma interface de map para ligar um tipo T qualquer
(&str por exemplo) ao enum do Pseudo

O mesmo funcionara para o directive, mas o metodo a ser implementado seguira as
seguintes linhas:
	fn translate(&self, Keyword, Args) -> Vec<u8>

Isso permitira que o enum Token não dependa mais de entidades inteiramente
implementadas pelo codigo chamante.

Isso por sua vez permitira que o tipo Token se torne algo definido no arquivo
lexer.rs, ja que nao precisara mais de tipos parametrizados.

Tudo isso por sua vez vai facilitar a implementação do parser, visto que o tipo
token podera ser utilizado diretamente nele.



ON MEMORY AND MACHINE ENDIANNESS 
* FIX how memory gets interpreted (in terms of endianness) by the debugger!
> maybe the machine will need this additional information (memory endianness
> and target endianness)
>> this problem needs to be solved at the memory level... one solution would be
>> to require the callee to specify the source endianness of the data being
>> written and the target endianness. Then, when reading data, the callee would
>> only have to specify the target endianness

>> the machine might expect to handle data in a certain endianness which has
>> nothing to do with the endianness used by the memory itself to store the
>> data. Therefore, the design chosen opts to let the machine and memory have
>> different endianness, so that they handle data however they want. The memory
>> stores data according to the memory endianness (defined during its
>> instantiation), and can retrieve data with the same endianness that was
>> stored or a different one. this allows a machine (which expects data to be
>> in a different endianness) to store the data on the memory and ask for it to
>> give that same data back with the endianness that it understands. This
>> allows for a complete separation of concerns.
